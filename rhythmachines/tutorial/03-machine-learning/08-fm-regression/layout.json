[{
  "drag": {
    "top": null,
    "left": null,
    "dragging": false
  },
  "resize": {
    "width": null,
    "height": null,
    "resizing": false
  },
  "responsive": {
    "valueW": 0
  },
  "static": false,
  "resizable": true,
  "draggable": true,
  "min": {},
  "max": {},
  "x": 0,
  "y": 0,
  "w": 6,
  "h": 7,
  "id": "_iig58jm02",
  "type": "liveCodeEditor",
  "name": "hello-world",
  "background": "#151515",
  "lineNumbers": true,
  "hasFocus": false,
  "theme": "icecoder",
  "data": "//this is a two-operator FM synthesis model\n\n//send the mouse to the ML window\n{{25}imp, 0, [{}mouseX, {}mouseY],2}toJS;\n\n\n//feedback connection\n:feedback:{:opB:, {{{6}fromJS}abs,3}pow}mul;\n\n//operator 1\n//base frequency, modulated by feedback\n:freqA:{{{0}fromJS, :feedback:}add,20,10000}uexp;\n//modulation frequency\n:freqA2:{{1}fromJS,1000}mul;\n//modulation index\n:modIdxA:{{2}fromJS, 200}mul;\n//modulator\n:modA:{{:freqA2:}sin,:modIdxA:}mul;\n//carrier\n:opA:{{:freqA:,:modA:}add}sin;\n\t\t\t\t\n//operator 2\n//base frequency, modulated by operator 1\n:freqB:{{{3}fromJS,:opA:}add,20,12000}uexp;\n:freqB2:{{4}fromJS,1000}mul;\n:modIdxB:{{5}fromJS, 200}mul;\n:modB:{{:freqB2:}sin,:modIdxB:}mul;\n:opB:{{:freqB:,:modB:}add}sin;\n\n//a bit of distortion on the output\n>{:opB:, 0.5, 0.8}asymclip;",
  "grammarSource": "/languages/default/grammar.ne",
  "liveCodeSource": "/languages/default/code.sem",
  "grammar": "\n# Lexer [or tokenizer] definition with language lexemes [or tokens]\n@{%\n\nconst lexer = moo.compile({\n  separator:      /,/,\n  paramEnd:       /}/,\n  paramBegin:     /{/,\n  listEnd:        /\\]/,\n  listBegin:      /\\[/,\n  dacoutCh:       /\\>[0-9]+/,\n  dacout:         /\\>/,\n  variable:       /:[a-zA-Z0-9]+:/,\n  sample:         { match: /\\\\[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  slice:          { match: /\\|[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  stretch:        { match: /\\@[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  clockTrig:      /0t-?(?:[0-9]|[1-9][0-9]+)(?:\\.[0-9]+)?\\b/,\n\tnumber:         /-?(?:[0-9]|[1-9][0-9]+)(?:\\.[0-9]+)?\\b/,\n  semicolon:      /;/,\n  funcName:       /[a-zA-Z][a-zA-Z0-9]*/,\n\tstring:\t\t\t\t\t{ match: /'[a-zA-Z0-9]+'/, value: x => x.slice(1,x.length-1)},\n  comment:        /\\/\\/[^\\n]*/,\n  ws:             { match: /\\s+/, lineBreaks: true},\n});\n\n%}\n\n# Pass your lexer object using the @lexer option\n@lexer lexer\n\n# Grammar definition in the Extended Backus Naur Form (EBNF)\nmain -> _ Statement _\n{% d => ( { '@lang' : d[1] } )  %}\n\nStatement ->\n  %comment _ Statement\n  {% d => d[2] %}\n\t|\n  Expression _ %semicolon _ Statement\n  {% d => [ { '@spawn': d[0] } ].concat(d[4]) %}\n  |\n  Expression _ %semicolon (_ %comment):*\n  {% d => [ { '@spawn': d[0] } ] %}\n\n\nExpression ->\n  ParameterList _ %funcName\n  {% d => sema.synth( d[2].value, d[0]['@params'] ) %}\n  |\n  ParameterList _ %sample\n  {% d => sema.synth( 'sampler', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  ParameterList _ %slice\n  {% d => sema.synth( 'slice', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  ParameterList _ %stretch\n  {% d => sema.synth( 'stretch', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  %variable _ Expression\n  {% d => sema.setvar( d[0].value, d[2] ) %}\n  |\n  %dacout _ Expression\n  {% d => sema.synth( 'dac', [d[2]] ) %}\n  |\n  %dacoutCh _ Expression\n  {% d => sema.synth( 'dac', [d[2], sema.num(d[0].value.substr(1))] ) %}\n\nParameterList ->\n  %paramBegin Params %paramEnd\n  {% d => ( { 'paramBegin': d[0], '@params': d[1], 'paramEnd': d[2] } ) %}\n\t|\n\t%paramBegin _ %paramEnd\n  {% d => ( { 'paramBegin': d[0], '@params': [], 'paramEnd': d[2] } ) %}\n\n\nParams ->\n  ParamElement\n  {% d => ( [ d[0] ] ) %}\n  |\n  ParamElement _ %separator _ Params\n  {% d => [ d[0] ].concat(d[4]) %}\n\nParamElement ->\n  %number\n  {% d => ( { '@num': d[0] } ) %}\n\t|\n\t%string\n  {% d => ( { '@string': d[0].value } ) %}\n  |\n  Expression\n  {% id %}\n  |\n  %variable\n  {% d => sema.getvar( d[0].value ) %}\n  |\n  %listBegin Params  %listEnd\n  {% d => ( { '@list': d[1] } )%}\n\n\n# Whitespace\n\n_  -> wschar:*\n{% function(d) {return null;} %}\n\n__ -> wschar:+\n{% function(d) {return null;} %}\n\nwschar -> %ws\n{% id %}\n"
}, {
  "drag": {
    "top": null,
    "left": null,
    "dragging": false
  },
  "resize": {
    "resizing": false,
    "width": 0,
    "height": 0
  },
  "responsive": {
    "valueW": 0
  },
  "static": false,
  "resizable": true,
  "draggable": true,
  "min": {},
  "max": {},
  "x": 9,
  "y": 0,
  "w": 6,
  "h": 7,
  "id": "_0d8t7cuqh",
  "name": "hello world",
  "type": "modelEditor",
  "lineNumbers": true,
  "hasFocus": true,
  "theme": "monokai",
  "background": "#f0f0f0",
  "data": "//:::1::: RUN THIS BLOCK OF CODE FIRST, THEN SCROLL DOWN TO STEP 2\n\n//these are the parameters that will be sent to the FM synth\nvar state = [0.01, 0.2, 0.0, 0.1, 0.1, 0.3, 0.1];\n\n//track the mouse\nvar mouseX=0;\nvar mouseY=0;\n\n//modes for machine learning\nvar MLMODES = {NONE:0,TRAIN:1,PREDICT:2};\nvar mode = MLMODES.NONE;\n\n//setup channels to send data to LC\nvar outChannels = [];\nfor(let i=0; i < 7; i++) {\n\toutChannels[i]= createOutputChannel(i, 1);\n}\n\n//a function to set data to the LC window\nfunction sendState(st) {\n\tfor(i in outChannels) {\n\t\toutChannels[i].send(st[i]);\n\t}\n}\n\n\n//record mouse positions for training data\nvar mousePositions = [];\n\n//record states as targets for the model\nvar states = [];\nvar inputMap = {\n\t0:(x)=>{\n\t\t//record the mouse values\n\t\tmouseX = x[0]; mouseY = x[1];\n\t\tif (mode == MLMODES.TRAIN) {\n\t\t\t//collect training data\n\t\t\tmousePositions.push([mouseX, mouseY]);\t\n\t\t\tstates.push(state);\n\t\t\tconsole.log(mousePositions.length, \"samples collected\");\n\t\t}else if (mode == MLMODES.PREDICT) {\n\t\t\t//make a prediction from the model\n\t\t\tlet modelInput = tf.tensor2d([mouseX, mouseY],[1,2]);\n\t\t\tlet prediction = model.predict(modelInput).dataSync();\n\t\t\tsendState(prediction);\n\t\t}\n\t},\n};\n\n//receive data from the LC window\ninput = (id,x) => {\n\tif (inputMap[id]) {\n\t\tinputMap[id](x);\n\t}\n};\n\n//this function generates a random list of numbers.We'll use it to explore the parameter space\nfunction randomList(n) {\n\tlet x = [];\n\tfor(let i=0; i < n; i++) x.push(Math.random());\n\treturn x;\n}\n\n_____\n\n//:::2::: MANUAL CONTROL OF THE FM SYNTH\n\n// try changing the state by changing the numbers below\n// they need to be between 0 and 1\nstate = [0.2, 0.9, 0.9, 0.0, 0.3, 0.2, 0.0];\nsendState(state);\nstate\n_____\n\n//:::3::: RANDOM EXPLORATION OF THE SYNTH\n\n//run this code block to randomise the parameters - repeat until you find a sound that you'd like to train as a target for the network\nstate = randomList(7);\nsendState(state);\nstate\n\n___\n\n//:::4::: TRAINING DATA COLLECTION\n//To train the synth we're going to match up mouse coordinates to parameter settings. During training, you'll need to control the cursor position in this editor with the keyboard.\n\n\n//:::4a::: move the mouse to the top left corner of the screen\n\n\n____\n//:::4b::: run this block to collect data, move the mouse around the top left corner for around 100-200 steps. You will see in the console window that data is being collected\nmode = MLMODES.TRAIN;\n___\n//:::4c::: run this block to stop collecting data\nmode = MLMODES.NONE;\n//:::4d:::repeat from step 3, with the mouse in the bottom right area of the screen\n___\n\n\n_____\n//:::5::: MODEL CREATION\n\n//create a neural network model.\nvar model = tf.sequential();\n//two inputs: mouse x and y\n//7 outputs - the parameters to send to our synth\n//one layer in between - we don't need more because this - it is a simple linear mapping\nmodel.add(tf.layers.dense({\n  inputShape: [2],\n  units: 30,\n  activation: 'relu',\n  kernelInitialiser: 'leCunNormal' }));\nmodel.add(tf.layers.dense({ units: 30 }));\nmodel.add(tf.layers.dense({ units: outChannels.length }));\nmodel.compile({ loss: 'meanSquaredError', optimizer: tf.train.adam() });\n\n\n____\n\n//:::6::: DATA CONVERSION\n\n//Convert the data we've collected into the right format for tensorflow\nvar trainIn = tf.tensor2d(mousePositions, [mousePositions.length, 2]);\nvar trainOut = tf.tensor2d(states, [states.length, state.length]);\n\n____\n\n//:::7::: TRAINING\n//train the model.  You can repeat this step as desired until the loss is low enough - 50 epochs is probably enough\n\nfunction onBatchEnd (x, logs){console.log(x, logs)};\n\nmodel.fit(trainIn, trainOut, { epochs: 50, callbacks: {onBatchEnd} }).then(info => {console.log(info);});\n\n_____\n\n//try a manual prediction\nmodel.predict(tf.tensor2d([mouseX, mouseY],[1,2])).dataSync()\n\n\n____\n//:::8::: PREDICTION\n//use the model to predict synthesis parameters based on the mouse movements\n//run this block then move the mouse around\n//you should find something similar to your original two sounds in the top-left and bottom-right corners.  All other areas in between are interpolations from the model - you might find some interesting new sounds\nmode = MLMODES.PREDICT;\n\n___\n//turn off prediction if needed\nmode = MLMODES.NONE;\n\n\n\n\n\n\n"
}, {
  "drag": {
    "top": null,
    "left": null,
    "dragging": false
  },
  "resize": {
    "width": null,
    "height": null,
    "resizing": false
  },
  "responsive": {
    "valueW": 0
  },
  "static": false,
  "resizable": true,
  "draggable": true,
  "min": {},
  "max": {},
  "x": 6,
  "y": 2,
  "w": 3,
  "h": 5,
  "id": "_78x4p8n6u",
  "name": "hello world",
  "type": "liveCodeParseOutput",
  "lineNumbers": false,
  "hasFocus": false,
  "theme": "shadowfox",
  "background": "#ebdeff",
  "data": ""
}]
