[{
  "drag": {
    "top": null,
    "left": null,
    "dragging": false
  },
  "resize": {
    "width": null,
    "height": null,
    "resizing": false
  },
  "responsive": {
    "valueW": 0
  },
  "static": false,
  "resizable": true,
  "draggable": true,
  "min": {},
  "max": {},
  "x": 0,
  "y": 0,
  "w": 6,
  "h": 7,
  "id": "_iig58jm02",
  "type": "liveCodeEditor",
  "name": "hello-world",
  "background": "#151515",
  "lineNumbers": true,
  "hasFocus": true,
  "theme": "icecoder",
  "data": "bpm 150;\nsource kick*2;\nsnare*2;\nclosedhat*1 1;\n",
  "grammarSource": "/languages/rubberGosling/grammar.ne",
  "liveCodeSource": "/languages/rubberGosling/code.sem",
  "grammar": "# GRAMMAR EDITOR\n\n# Lexer [or tokenizer] definition with language lexemes [or tokens]\n@{%\n\nconst lexer = moo.compile({\n  separator:      /,/,\n  paramEnd:       /}/,\n  paramBegin:     /{/,\n  listEnd:        /\\]/,\n  listBegin:      /\\[/,\n  dacoutCh:       /\\>[0-9]+/,\n  dacout:         /\\>/,\n  variable:       /:[a-zA-Z0-9]+:/,\n  sample:         { match: /\\\\[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  slice:          { match: /\\|[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  stretch:        { match: /\\@[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  clockTrig:      /0t-?(?:[0-9]|[1-9][0-9]+)(?:\\.[0-9]+)?\\b/,\n\tnumber:         /-?(?:[0-9]|[1-9][0-9]+)(?:\\.[0-9]+)?\\b/,\n  semicolon:      /;/,\n  funcName:       /[a-zA-Z][a-zA-Z0-9]*/,\n\tstring:\t\t\t\t\t{ match: /'[a-zA-Z0-9]+'/, value: x => x.slice(1,x.length-1)},\n  comment:        /\\/\\/[^\\n]*/,\n  ws:             { match: /\\s+/, lineBreaks: true},\n});\n\n%}\n\n# Pass your lexer object using the @lexer option\n@lexer lexer\n\n# Grammar definition in the Extended Backus Naur Form (EBNF)\nmain -> _ Statement _\n{% d => ( { '@lang' : d[1] } )  %}\n\nStatement ->\n  %comment _ Statement\n  {% d => d[2] %}\n\t|\n  Expression _ %semicolon _ Statement\n  {% d => [ { '@spawn': d[0] } ].concat(d[4]) %}\n  |\n  Expression _ %semicolon (_ %comment):*\n  {% d => [ { '@spawn': d[0] } ] %}\n\n\nExpression ->\n  ParameterList _ %funcName\n  {% d => sema.synth( d[2].value, d[0]['@params'] ) %}\n  |\n  ParameterList _ %sample\n  {% d => sema.synth( 'sampler', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  ParameterList _ %slice\n  {% d => sema.synth( 'slice', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  ParameterList _ %stretch\n  {% d => sema.synth( 'stretch', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  %variable _ Expression\n  {% d => sema.setvar( d[0].value, d[2] ) %}\n  |\n  %dacout _ Expression\n  {% d => sema.synth( 'dac', [d[2]] ) %}\n  |\n  %dacoutCh _ Expression\n  {% d => sema.synth( 'dac', [d[2], sema.num(d[0].value.substr(1))] ) %}\n\nParameterList ->\n  %paramBegin Params %paramEnd\n  {% d => ( { 'paramBegin': d[0], '@params': d[1], 'paramEnd': d[2] } ) %}\n\t|\n\t%paramBegin _ %paramEnd\n  {% d => ( { 'paramBegin': d[0], '@params': [], 'paramEnd': d[2] } ) %}\n\n\nParams ->\n  ParamElement\n  {% d => ( [ d[0] ] ) %}\n  |\n  ParamElement _ %separator _ Params\n  {% d => [ d[0] ].concat(d[4]) %}\n\nParamElement ->\n  %number\n  {% d => ( { '@num': d[0] } ) %}\n\t|\n\t%string\n  {% d => ( { '@string': d[0].value } ) %}\n  |\n  Expression\n  {% id %}\n  |\n  %variable\n  {% d => sema.getvar( d[0].value ) %}\n  |\n  %listBegin Params  %listEnd\n  {% d => ( { '@list': d[1] } )%}\n\n\n# Whitespace\n\n_  -> wschar:*\n{% function(d) {return null;} %}\n\n__ -> wschar:+\n{% function(d) {return null;} %}\n\nwschar -> %ws\n{% id %}\n"
}, {
  "drag": {
    "dragging": false,
    "top": 0,
    "left": 0
  },
  "resize": {
    "resizing": false,
    "width": 0,
    "height": 0
  },
  "responsive": {
    "valueW": 0
  },
  "static": false,
  "resizable": true,
  "draggable": true,
  "min": {},
  "max": {},
  "x": 8,
  "y": 0,
  "w": 7,
  "h": 7,
  "id": "_0d8t7cuqh",
  "name": "hello world",
  "type": "modelEditor",
  "lineNumbers": true,
  "hasFocus": false,
  "theme": "monokai",
  "background": "#f0f0f0",
  "data": "// STEP 1:  RUN THIS SCRIPT\n// STEP 2:  START LIVECODING\n\nvar structure = [32,16,8];\nvar replayMemory = 64;\nvar replaySize = 32;\nvar beatCount=8;\nvar agentShortTermMemory = 0; //set this as a percentage of the beatcount\n\n\n\n\n\n\n\nvar agentSTMSize = Math.floor(beatCount * agentShortTermMemory);\nimportScripts('https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.15/lodash.js');\ntf.setBackend('cpu');\n\nvar createMemory = (maxMem) => {\n\tlet mem = {}\n\tmem.samples=[];\n\tmem.maxMemory= maxMem;\n\tmem.test= () => console.log(mem);\n\tmem.addSample = (sample) => {\n\t\t\tmem.samples.push(sample);\n\t\t\tif (mem.samples.length > mem.maxMemory) {mem.samples.shift()}\n\t\t};\n\tmem.sample = (n) => {\n\t\t\treturn _.sampleSize(mem.samples, n);\n\t\t};\n\treturn mem;\n}\n\nvar createModel = () => {\n\tlet model = {};\n\tmodel.init = (hiddenLayerSizes, numStates, numActions, batchSize, memSize) => {\n\t\tmodel.net = tf.sequential();\n\t\tmodel.numStates = numStates;\n\t\tmodel.numActions = numActions;\n\t\tmodel.net.add(tf.layers.dense({\n\t\t\tunits:numStates,\n\t\t\tactivation: 'tanh',\n\t\t\tinputShape: [numStates]\n\t\t}));\n\t\tmodel.net.add(tf.layers.dropout({rate: 0.2}))\n\t\thiddenLayerSizes.forEach((layerSize,i) => {\n\t\t\tmodel.net.add(tf.layers.dense({\n\t\t\t\tunits:layerSize,\n\t\t\t\tactivation: 'tanh',\n\t\t\t\tinputShape: undefined\n\t\t\t}));\t\n\t\t});\n\t\tmodel.net.add(tf.layers.dense({units:numActions}));\n\t\tmodel.net.summary();\n\t\tmodel.net.compile({optimizer:'adam', loss:'meanSquaredError'});\n\t\tmodel.memory = createMemory(memSize);\n\t};\n\tmodel.predict = (state) => {\n\t\treturn tf.tidy(()=>{return model.net.predict(state)});\n\t};\n\tmodel.nextAction = (state,eps) => {\n\t\tlet action=0;\n\t\tif (Math.random() < eps) {\n\t\t\taction = Math.floor(Math.random() * model.numActions);\n\t\t}else{\n\t\t\taction = tf.tidy(()=>{\n\t\t\t\treturn model.net.predict(state).argMax(1).dataSync()[0];\n\t\t\t}); \n\t\t}\n\t\treturn action;\n\t};\n\tmodel.train = () => {\n\t\n\t};\n\treturn model;\n};\n\n/////////////////////////////////////////////////MODEL INIT\nvar model = createModel();\n\nmodel.init(structure, beatCount + beatCount + agentSTMSize, 2, 4, replayMemory);\n\n\nvar createEnvironment = (beatCount, agentSTMSize) => {\n\tlet env = [];\n\tenv.timeEncodingSize = beatCount;\n\tenv.beatCount = beatCount;\n\tenv.agentSTMSize = agentSTMSize;\n\tenv.stateSize = env.timeEncodingSize + env.beatCount + env.agentSTMSize;\n\tenv.createInitState = () => {\n\t\t//state = memory of the environment ++ memory of past actions\n\t\tlet newState = new Array(env.stateSize).fill(-1);\n\t\treturn newState; \n\t}\n\tenv.update = (currState, beatTime, input, action) =>{\n\t\tlet newState = [];\n\t\t//time encoding\n\t\tfor(let i=0; i < env.beatCount; i++) {\n\t\t\tnewState.push(i==beatTime ? 1 : -1);\n\t\t}\n\t\t//input history\n\t\t//shift agent memory\n\t\tfor(let i=env.beatCount+1; i < env.beatCount*2; i++) {\n\t\t\tnewState[i] = currState[i-1];\n\t\t}\n\t\tnewState[env.beatCount] = input;\n\t\t\n\t\t//agent STM\n\t\tif (env.agentSTMSize > 0) {\n\t\t\tfor(let i=(env.beatCount*2)+1; i < env.stateSize; i++) {\n\t\t\t\tnewState[i] = currState[i-1];\n\t\t\t}\n\t\t\tnewState[env.beatCount*2] = action ? 1 : -1;\n\t\t}\n\t\treturn newState;\n\t}\n\treturn env;\n}\n\n\nvar env = createEnvironment(beatCount, agentSTMSize);\n\nvar state = env.createInitState();\n\nvar lastAction=0;\n\nvar step = async (currState, beatNum, input, expectedoutput, sampleSize, eps, learning) => {\n\t\n\tlet action, reward, newState;\n\tif (learning) {\n\t\taction =model.nextAction(tf.tensor2d(currState,[1,currState.length]),eps);\n\t\t//console.log(\"exp: \", expectedoutput, \", action: \", action);\n\t\treward = expectedoutput == action ? 1 : -1;\n\t}else{\n\t\taction=lastAction;\n\t}\n\t\n\tnewState = env.update(currState, beatNum, input, action);\n\t//console.log(\"Curr: \", currState, \"New\", newState, \"Action: \", action, \"Reward: \", reward);\n\t\n\tif(!learning) {\n\t\taction =model.nextAction(tf.tensor2d(newState,[1,newState.length]),eps);\n\t}\n\t\n\tlastAction=action;\n\t\n\tif (learning) {\n\t\tmodel.memory.addSample([currState, reward, action, newState]);\n\n\t\t//calc rewards\n\t\tlet x=[];\n\t\tlet targets=[];\n\t\tlet memSamples = model.memory.sample(sampleSize);\n\t\tlet debug=[];\n\t\tif (memSamples.length >0) {\n\t\t\tmemSamples.forEach(\n\t\t\t\t([sampleState, sampleReward, sampleAction, sampleNextState],i) => {\n\t\t\t\t\ttf.tidy(()=>{\n\t\t\t\t\t\tlet val = model.predict(tf.tensor2d(sampleState, [1,sampleState.length])).dataSync();\n\t\t\t\t\t\tlet maxValNextState = model.predict(tf.tensor2d(sampleNextState, [1,sampleNextState.length])).max().dataSync()[0];\n\t\t\t\t\t\tval[sampleAction] = sampleReward; \n\t\t\t\t\t\t\n\t\t\t\t\t\tx.push(sampleState);\n\t\t\t\t\t\ttargets.push(val);\n//\t\t\t\t\t\tdebug.push([sampleState[0], sampleState[1], sampleAction, sampleReward, val[0], val[1]]);\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t);\n//\t\t\tconsole.table(debug);\n\t\t\t//learn\n\t\t\tlet xtf = tf.tensor2d(x, [x.length, model.numStates]);\n\t\t\tlet ytf = tf.tensor2d(targets, [targets.length, model.numActions]);\n\t\t\tfunction onBatchEnd (x, logs){\n\t\t\t};\n\t\t\tawait model.net.fit(xtf,ytf, {batchSize:32, epochs: 1, callbacks: {onBatchEnd} }).then(info => {console.log('loss', info.history.loss);});\n\t\t}\n\t}\n\treturn [newState, action, reward];\n}\n\nvar trigOut = createOutputChannel(0, 1);\nvar nextAction = 0;\nvar runningReward = 0;\n\ninput = async (id,x) => {\n\t//console.log(id,x);\n\tlet targetInput=x[1];\n\tlet inputVal = x[0] ? 1 : -1;\n\tlet beatNum = Math.floor(x[2]*beatCount);\n\tlet memSampleSize=replaySize;\n\tif (targetInput ==-1) {\n\t\t//we're in prediction mode\n\t\t[state, nextAction] = await step(state, beatNum, inputVal, 0, memSampleSize, 0.0, 0);\n\t\t\n\t\ttrigOut.send(nextAction);\n\t\tconsole.log('sent prediction',nextAction);\n\t}else{\n\t\t//we're learning\n\t\tlet expectedAction = x[1];\n\t\t[state, nextAction, r] = await step(state, beatNum, inputVal, expectedAction, memSampleSize, 0.0, 1);\n\t\trunningReward = (0.9 * runningReward) + (0.1 * r);\n\t\t//console.log(\"R\", runningReward);\n\t}\n}\n\n//references...\n//https://medium.com/@pierrerouhard/reinforcement-learning-in-the-browser-an-introduction-to-tensorflow-js-9a02b143c099\n//https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8/\n\n\n"
}, {
  "drag": {
    "dragging": false,
    "top": 0,
    "left": 0
  },
  "resize": {
    "resizing": false,
    "width": 0,
    "height": 0
  },
  "responsive": {
    "valueW": 0
  },
  "static": false,
  "resizable": true,
  "draggable": true,
  "min": {},
  "max": {},
  "x": 6,
  "y": 0,
  "w": 2,
  "h": 7,
  "id": "_iwcx7vn7t",
  "type": "liveCodeParseOutput",
  "name": "liveCodeParseOutput_iwcx7vn7t",
  "lineNumbers": true,
  "hasFocus": false,
  "background": "#ebdeff"
}]
